{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e02ae64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20% subset\n",
      "Processing 40% subset\n",
      "Processing 60% subset\n",
      "Processing 80% subset\n",
      "Processing 100% subset\n",
      "Available subsets: ['20', '40', '60', '80', '100']\n",
      "\n",
      "20% subset head:\n",
      "          matchID  fullTimeMS  timePercent  winnerMinionsKilled  \\\n",
      "0  BR1_2720891721     2092233            6                  109   \n",
      "1  BR1_2720337066     1867984            6                  131   \n",
      "2  BR1_2720218416     2396948            7                  162   \n",
      "3  BR1_2720199652     1656791            5                  117   \n",
      "4  BR1_2720058177     1875549            6                  119   \n",
      "\n",
      "   loserMinionsKilled  winnerChampionKill  loserChampionKill  \\\n",
      "0                  87                   5                  3   \n",
      "1                 114                   3                  1   \n",
      "2                 130                   3                  1   \n",
      "3                 111                   2                  1   \n",
      "4                 120                   5                  2   \n",
      "\n",
      "   winnerDragonKill  loserDragonKill  winnerDragonElderKill  ...  \\\n",
      "0                 0                0                      0  ...   \n",
      "1                 0                0                      0  ...   \n",
      "2                 0                0                      0  ...   \n",
      "3                 0                0                      0  ...   \n",
      "4                 0                0                      0  ...   \n",
      "\n",
      "   loserRiftHeraldKill  winnerBaronKill  loserBaronKill  winnerTowerKill  \\\n",
      "0                    0                0               0                0   \n",
      "1                    0                0               0                0   \n",
      "2                    0                0               0                0   \n",
      "3                    0                0               0                0   \n",
      "4                    0                0               0                0   \n",
      "\n",
      "   loserTowerKill  winnerInhibitorKill  loserInhibitorKill  winnerTotalGold  \\\n",
      "0               0                    0                   0             9299   \n",
      "1               0                    0                   0             9018   \n",
      "2               0                    0                   0            10890   \n",
      "3               0                    0                   0             8073   \n",
      "4               0                    0                   0            10378   \n",
      "\n",
      "   loserTotalGold  subset  \n",
      "0            8014      20  \n",
      "1            7846      20  \n",
      "2            9085      20  \n",
      "3            7220      20  \n",
      "4            8438      20  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "100% subset info:\n",
      "          matchID  fullTimeMS  timePercent  winnerMinionsKilled  \\\n",
      "0  BR1_2720891721     2092233           35                  650   \n",
      "1  BR1_2720337066     1867984           32                  626   \n",
      "2  BR1_2720218416     2396948           40                  880   \n",
      "3  BR1_2720199652     1656791           28                  696   \n",
      "4  BR1_2720058177     1875549           32                  652   \n",
      "\n",
      "   loserMinionsKilled  winnerChampionKill  loserChampionKill  \\\n",
      "0                 618                  44                 44   \n",
      "1                 567                  45                 27   \n",
      "2                 816                  36                 33   \n",
      "3                 608                  34                 17   \n",
      "4                 648                  50                 19   \n",
      "\n",
      "   winnerDragonKill  loserDragonKill  winnerDragonElderKill  ...  \\\n",
      "0                 3                2                      0  ...   \n",
      "1                 3                1                      0  ...   \n",
      "2                 2                3                      0  ...   \n",
      "3                 4                0                      0  ...   \n",
      "4                 3                1                      0  ...   \n",
      "\n",
      "   loserRiftHeraldKill  winnerBaronKill  loserBaronKill  winnerTowerKill  \\\n",
      "0                    1                0               1               11   \n",
      "1                    1                1               0                7   \n",
      "2                    1                0               2               10   \n",
      "3                    0                0               0               11   \n",
      "4                    2                0               0                6   \n",
      "\n",
      "   loserTowerKill  winnerInhibitorKill  loserInhibitorKill  winnerTotalGold  \\\n",
      "0               2                    3                   0            65209   \n",
      "1               3                    2                   0            60003   \n",
      "2               6                    2                   0            72823   \n",
      "3               2                    2                   0            56083   \n",
      "4               5                    1                   2            62117   \n",
      "\n",
      "   loserTotalGold  subset  \n",
      "0           62978     100  \n",
      "1           52003     100  \n",
      "2           69358     100  \n",
      "3           43739     100  \n",
      "4           48850     100  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Converts blue-red into win-lose\n",
    "def process_data(data):\n",
    "    \"\"\"Vectorized processing of a single dataframe\"\"\"\n",
    "    blue_wins = data['blueWin'] == 1\n",
    "    \n",
    "    # Initialize new dataframe with common columns\n",
    "    new_df = data[['matchID', 'fullTimeMS', 'timePercent']].copy()\n",
    "    \n",
    "    # Define core metrics to process\n",
    "    metrics = [\n",
    "        'ChampionKill',\n",
    "        'DragonKill',\n",
    "        'DragonElderKill',\n",
    "        'RiftHeraldKill',\n",
    "        'BaronKill',\n",
    "        'TowerKill',\n",
    "        'InhibitorKill',\n",
    "        'TotalGold'\n",
    "    ]\n",
    "    \n",
    "    # Process combined minions\n",
    "    new_df['winnerMinionsKilled'] = np.where(\n",
    "        blue_wins,\n",
    "        data['blueMinionsKilled'] + data['blueJungleMinionsKilled'],\n",
    "        data['redMinionsKilled'] + data['redJungleMinionsKilled']\n",
    "    )\n",
    "    new_df['loserMinionsKilled'] = np.where(\n",
    "        blue_wins,\n",
    "        data['redMinionsKilled'] + data['redJungleMinionsKilled'],\n",
    "        data['blueMinionsKilled'] + data['blueJungleMinionsKilled']\n",
    "    )\n",
    "    \n",
    "    # Process all core metrics\n",
    "    for metric in metrics:\n",
    "        new_df[f'winner{metric}'] = np.where(\n",
    "            blue_wins,\n",
    "            data[f'blue{metric}'],\n",
    "            data[f'red{metric}']\n",
    "        )\n",
    "        new_df[f'loser{metric}'] = np.where(\n",
    "            blue_wins,\n",
    "            data[f'red{metric}'],\n",
    "            data[f'blue{metric}']\n",
    "        )\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# Main processing\n",
    "data_subsets = {\n",
    "    '20': 'full_data_20.csv',\n",
    "    '40': 'full_data_40.csv',\n",
    "    '60': 'full_data_60.csv',\n",
    "    '80': 'full_data_80.csv',\n",
    "    '100': 'full_data_100.csv'\n",
    "}\n",
    "\n",
    "# Dictionary to store processed DataFrames\n",
    "processed_datasets = {}\n",
    "\n",
    "for subset_label, file_path in data_subsets.items():\n",
    "    print(f\"Processing {subset_label}% subset\")\n",
    "    raw_data = pd.read_csv(file_path)\n",
    "    processed_data = process_data(raw_data)\n",
    "    \n",
    "    # Add subset label as a column for reference\n",
    "    processed_data['subset'] = subset_label\n",
    "    \n",
    "    # Store in dictionary\n",
    "    processed_datasets[subset_label] = processed_data\n",
    "\n",
    "# Example usage:\n",
    "print(f\"Available subsets: {list(processed_datasets.keys())}\")\n",
    "print(\"\\n20% subset head:\")\n",
    "print(processed_datasets['20'].head())\n",
    "print(\"\\n100% subset info:\")\n",
    "print(processed_datasets['100'].head())\n",
    "\n",
    "# If you need one combined DataFrame:\n",
    "combined_df = pd.concat(processed_datasets.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78a90c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def create_gold_labels(data_dict):\n",
    "    subsets = sorted(data_dict.keys(), key=lambda x: int(x))\n",
    "    \n",
    "    metrics = [\n",
    "        'winnerChampionKill',\n",
    "        'winnerDragonKill',\n",
    "        'winnerDragonElderKill',\n",
    "        'winnerRiftHeraldKill',\n",
    "        'winnerBaronKill',\n",
    "        'winnerTowerKill',\n",
    "        'winnerInhibitorKill',\n",
    "        'winnerTotalGold',\n",
    "        'winnerMinionsKilled'\n",
    "    ]\n",
    "    \n",
    "    label_map = {\n",
    "        'winnerChampionKill': 'Kills',\n",
    "        'winnerDragonKill': 'Dragons',\n",
    "        'winnerDragonElderKill': 'Elder Dragon',\n",
    "        'winnerRiftHeraldKill': 'Herald',\n",
    "        'winnerBaronKill': 'Baron',\n",
    "        'winnerTowerKill': 'Towers',\n",
    "        'winnerInhibitorKill': 'Inhibitors',\n",
    "        'winnerTotalGold': 'Gold',\n",
    "        'winnerMinionsKilled': 'Minions'\n",
    "    }\n",
    "\n",
    "    all_possible_labels = list(label_map.values())\n",
    "\n",
    "    gold_labels = {}\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_possible_labels)\n",
    "\n",
    "    for i, subset in enumerate(subsets):\n",
    "        current_data = data_dict[subset]\n",
    "        \n",
    "        # Create DataFrame to store results (preserves matchID and order)\n",
    "        result_df = pd.DataFrame(index=current_data.index)\n",
    "        \n",
    "        if subset == '20':\n",
    "            # For 20%: curr - 0\n",
    "            differences = current_data[metrics].values\n",
    "        else:\n",
    "            # For others: curr - prev\n",
    "            prev_data = data_dict[subsets[i-1]]\n",
    "\n",
    "            # Since match IDs are in same order, we can directly subtract\n",
    "            current_values = current_data[metrics].values\n",
    "            prev_values = prev_data[metrics].values  # Direct access in same order\n",
    "\n",
    "            # Calculate differences (current - previous)\n",
    "            differences = current_values - prev_values\n",
    "        \n",
    "        # Normalize and label\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized = scaler.fit_transform(differences)\n",
    "        \n",
    "        # Store results\n",
    "        result_df['gold_label'] = [label_map[metrics[np.argmax(row)]] for row in normalized]\n",
    "\n",
    "        gold_labels[subset] = result_df\n",
    "\n",
    "    return gold_labels, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34384430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  gold_label\n",
      "0    Dragons\n",
      "1    Dragons\n",
      "2    Minions\n",
      "3     Herald\n",
      "4    Dragons\n",
      "  gold_label\n",
      "0     Towers\n",
      "1      Kills\n",
      "2    Dragons\n",
      "3    Dragons\n",
      "4      Kills\n"
     ]
    }
   ],
   "source": [
    "# Usage Example:\n",
    "gold_labels_final, label_encoder_final = create_gold_labels(processed_datasets)\n",
    "\n",
    "# Access labels for 40% subset:\n",
    "labels_40 = gold_labels['40']\n",
    "print(labels_40.head())\n",
    "\n",
    "labels_80 = gold_labels['80']\n",
    "print(labels_80.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34bfc9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# min/max scale the preprocessed data\n",
    "scaler = MinMaxScaler()\n",
    "# print(processed_datasets['20'].head(3))\n",
    "\n",
    "metrics = [\n",
    "    'ChampionKill',\n",
    "    'DragonKill',\n",
    "    'DragonElderKill',\n",
    "    'RiftHeraldKill',\n",
    "    'BaronKill',\n",
    "    'TowerKill',\n",
    "    'InhibitorKill',\n",
    "    'TotalGold',\n",
    "    'MinionsKilled'\n",
    "]\n",
    "\n",
    "\n",
    "# WILL NOT WORK TWICE IN A ROW\n",
    "subsets = sorted(processed_datasets.keys(), key=lambda x: int(x))\n",
    "for i, subset in enumerate(subsets):\n",
    "    current_data = processed_datasets[subset]\n",
    "    \n",
    "    # Create DataFrame to store results (preserves matchID and order)\n",
    "    result_df = pd.DataFrame(index=current_data.index)\n",
    "    \n",
    "\n",
    "    # Calculate differences (winner-loser)\n",
    "    for metric in metrics:\n",
    "        result_df[f'{metric}diff'] = current_data[f'winner{metric}'] - current_data[f'loser{metric}']\n",
    "    \n",
    "\n",
    "\n",
    "    # Normalize and label\n",
    "    scaler = MinMaxScaler()\n",
    "    result_df[result_df.columns] = scaler.fit_transform(result_df[result_df.columns])\n",
    "    \n",
    "    # Set new dataframe in place of old\n",
    "    processed_datasets[subset] = result_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ddc3694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X_train': array([[0.625     , 1.        , 0.        , ..., 0.        , 0.60924264,\n",
      "        0.42201835],\n",
      "       [0.5       , 0.5       , 0.        , ..., 0.        , 0.57988761,\n",
      "        0.63302752],\n",
      "       [0.54166667, 0.5       , 0.        , ..., 0.        , 0.6161201 ,\n",
      "        0.52293578],\n",
      "       ...,\n",
      "       [0.5       , 0.5       , 0.        , ..., 0.        , 0.55883586,\n",
      "        0.55963303],\n",
      "       [0.29166667, 0.5       , 0.        , ..., 0.        , 0.35351841,\n",
      "        0.41284404],\n",
      "       [0.54166667, 0.5       , 0.        , ..., 0.        , 0.56722301,\n",
      "        0.44036697]]), 'X_test': array([[0.625     , 0.5       , 0.        , ..., 0.        , 0.74159188,\n",
      "        0.69724771],\n",
      "       [0.54166667, 0.5       , 0.        , ..., 0.        , 0.61310073,\n",
      "        0.64678899],\n",
      "       [0.5       , 0.5       , 0.        , ..., 0.        , 0.54415835,\n",
      "        0.51376147],\n",
      "       ...,\n",
      "       [0.54166667, 0.5       , 0.        , ..., 0.        , 0.5652101 ,\n",
      "        0.55045872],\n",
      "       [0.58333333, 0.5       , 0.        , ..., 0.        , 0.63281053,\n",
      "        0.62844037],\n",
      "       [0.5       , 0.5       , 0.        , ..., 0.        , 0.55439067,\n",
      "        0.46330275]]), 'y_train': array([['Dragons'],\n",
      "       ['Minions'],\n",
      "       ['Minions'],\n",
      "       ...,\n",
      "       ['Minions'],\n",
      "       ['Minions'],\n",
      "       ['Gold']], dtype=object), 'y_test': array([['Minions'],\n",
      "       ['Minions'],\n",
      "       ['Minions'],\n",
      "       ...,\n",
      "       ['Minions'],\n",
      "       ['Minions'],\n",
      "       ['Minions']], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# split into train/test\n",
    "\n",
    "\n",
    "\n",
    "# Dictionary to store processed DataFrames\n",
    "train_test_dict = {\n",
    "    '20':{},\n",
    "    '40':{},\n",
    "    '60':{},\n",
    "    '80':{},\n",
    "    '100':{}\n",
    "}\n",
    "\n",
    "for i, subset in enumerate(subsets):\n",
    "    current_data = processed_datasets[subset]\n",
    "    current_gold = gold_labels[subset]\n",
    "\n",
    "    data_array = np.array(current_data)\n",
    "    gold_array = np.array(current_gold)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_array, gold_array, test_size=.2, random_state=42)\n",
    "\n",
    "    new_dict = {'X_train': X_train,\n",
    "                'X_test': X_test,\n",
    "                'y_train': y_train,\n",
    "                'y_test': y_test}\n",
    "    train_test_dict[subset] = new_dict \n",
    "\n",
    "\n",
    "print(train_test_dict['20'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d648b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaylawalley/miniconda3/envs/AI/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1124: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/shaylawalley/miniconda3/envs/AI/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/shaylawalley/miniconda3/envs/AI/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1124: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 20%: 0.807001239157373\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# # Create an MLPClassifier model\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, activation='relu', solver='adam', random_state=42)\n",
    "\n",
    "# for key in train_test_dict.keys():\n",
    "#   # print(key)\n",
    "#   subset = train_test_dict[key]\n",
    "#   # print(subset)\n",
    "\n",
    "#   # Train the model\n",
    "#   mlp.fit(subset['X_train'], subset['y_train'])\n",
    "\n",
    "#   # Make predictions on the test set\n",
    "#   y_pred = mlp.predict(subset['X_test'])\n",
    "\n",
    "#   # Evaluate the model\n",
    "#   accuracy = accuracy_score(subset['y_test'], y_pred)\n",
    "#   print(f\"Accuracy {key}%: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076fc2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x303922410>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def prepare_rnn_data(train_test_dict, batch_size=32):\n",
    "    tensor_dict = {}\n",
    "    \n",
    "    for subset in train_test_dict.keys():\n",
    "        data = train_test_dict[subset]\n",
    "        gold_labels = gold_labels_final[subset]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train = torch.FloatTensor(data['X_train']).unsqueeze(1)\n",
    "        X_test = torch.FloatTensor(data['X_test']).unsqueeze(1)\n",
    "        \n",
    "        # Get corresponding gold labels\n",
    "        # Since X_train/X_test are already split, we'll match by length\n",
    "        y_train = torch.LongTensor(label_encoder_final.transform(data['y_train'].flatten()))\n",
    "        y_test = torch.LongTensor(label_encoder_final.transform(data['y_test'].flatten()))\n",
    "        \n",
    "        # Verify lengths match\n",
    "        assert len(X_train) == len(y_train), f\"Train length mismatch in {subset}: X={len(X_train)}, y={len(y_train)}\"\n",
    "        assert len(X_test) == len(y_test), f\"Test length mismatch in {subset}: X={len(X_test)}, y={len(y_test)}\"\n",
    "        # Create datasets\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        \n",
    "        tensor_dict[subset] = {\n",
    "            'train_loader': DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
    "            'test_loader': DataLoader(test_dataset, batch_size=batch_size, shuffle=False),\n",
    "            'label_encoder': label_encoder_final\n",
    "        }\n",
    "    \n",
    "    return tensor_dict\n",
    "\n",
    "# Example usage\n",
    "tensor_data = prepare_rnn_data(train_test_dict, batch_size=64)\n",
    "\n",
    "# Access 20% data loaders\n",
    "train_loader_20 = tensor_data['20']['train_loader']\n",
    "test_loader_20 = tensor_data['20']['test_loader']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f8082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Train Loss: 1.3662, Val Loss: 1.2959, Val Acc: 0.4669\n",
      "Epoch 2/200\n",
      "Train Loss: 1.2279, Val Loss: 1.1915, Val Acc: 0.5153\n",
      "Epoch 3/200\n",
      "Train Loss: 1.1697, Val Loss: 1.2573, Val Acc: 0.4512\n",
      "Epoch 4/200\n",
      "Train Loss: 1.1473, Val Loss: 1.1391, Val Acc: 0.5125\n",
      "Epoch 5/200\n",
      "Train Loss: 1.1262, Val Loss: 1.1280, Val Acc: 0.5338\n",
      "Epoch 6/200\n",
      "Train Loss: 1.1132, Val Loss: 1.1094, Val Acc: 0.5386\n",
      "Epoch 7/200\n",
      "Train Loss: 1.0980, Val Loss: 1.1396, Val Acc: 0.5110\n",
      "Epoch 8/200\n",
      "Train Loss: 1.0876, Val Loss: 1.1127, Val Acc: 0.5286\n",
      "Epoch 9/200\n",
      "Train Loss: 1.0793, Val Loss: 1.0799, Val Acc: 0.5519\n",
      "Epoch 10/200\n",
      "Train Loss: 1.0767, Val Loss: 1.0801, Val Acc: 0.5485\n",
      "Epoch 11/200\n",
      "Train Loss: 1.0709, Val Loss: 1.0841, Val Acc: 0.5510\n",
      "Epoch 12/200\n",
      "Train Loss: 1.0662, Val Loss: 1.1012, Val Acc: 0.5415\n",
      "Epoch 13/200\n",
      "Train Loss: 1.0599, Val Loss: 1.0710, Val Acc: 0.5558\n",
      "Epoch 14/200\n",
      "Train Loss: 1.0606, Val Loss: 1.0838, Val Acc: 0.5587\n",
      "Epoch 15/200\n",
      "Train Loss: 1.0548, Val Loss: 1.0930, Val Acc: 0.5480\n",
      "Epoch 16/200\n",
      "Train Loss: 1.0534, Val Loss: 1.0868, Val Acc: 0.5298\n",
      "Epoch 17/200\n",
      "Train Loss: 1.0501, Val Loss: 1.0686, Val Acc: 0.5551\n",
      "Epoch 18/200\n",
      "Train Loss: 1.0478, Val Loss: 1.0684, Val Acc: 0.5560\n",
      "Epoch 19/200\n",
      "Train Loss: 1.0468, Val Loss: 1.0764, Val Acc: 0.5581\n",
      "Epoch 20/200\n",
      "Train Loss: 1.0445, Val Loss: 1.0619, Val Acc: 0.5582\n",
      "Epoch 21/200\n",
      "Train Loss: 1.0391, Val Loss: 1.0684, Val Acc: 0.5579\n",
      "Epoch 22/200\n",
      "Train Loss: 1.0384, Val Loss: 1.0674, Val Acc: 0.5584\n",
      "Epoch 23/200\n",
      "Train Loss: 1.0373, Val Loss: 1.0916, Val Acc: 0.5494\n",
      "Epoch 24/200\n",
      "Train Loss: 1.0357, Val Loss: 1.0802, Val Acc: 0.5580\n",
      "Epoch 25/200\n",
      "Train Loss: 1.0355, Val Loss: 1.0609, Val Acc: 0.5610\n",
      "Epoch 26/200\n",
      "Train Loss: 1.0304, Val Loss: 1.0619, Val Acc: 0.5591\n",
      "Epoch 27/200\n",
      "Train Loss: 1.0282, Val Loss: 1.0660, Val Acc: 0.5592\n",
      "Epoch 28/200\n",
      "Train Loss: 1.0277, Val Loss: 1.0642, Val Acc: 0.5602\n",
      "Epoch 29/200\n",
      "Train Loss: 1.0254, Val Loss: 1.0618, Val Acc: 0.5590\n",
      "Epoch 30/200\n",
      "Train Loss: 1.0243, Val Loss: 1.0689, Val Acc: 0.5575\n",
      "Epoch 31/200\n",
      "Train Loss: 1.0249, Val Loss: 1.0742, Val Acc: 0.5599\n",
      "Epoch 32/200\n",
      "Train Loss: 1.0184, Val Loss: 1.0709, Val Acc: 0.5573\n",
      "Epoch 33/200\n",
      "Train Loss: 1.0188, Val Loss: 1.0690, Val Acc: 0.5589\n",
      "Epoch 34/200\n",
      "Train Loss: 1.0169, Val Loss: 1.0731, Val Acc: 0.5543\n",
      "Epoch 35/200\n",
      "Train Loss: 1.0156, Val Loss: 1.0647, Val Acc: 0.5556\n",
      "Epoch 36/200\n",
      "Train Loss: 1.0147, Val Loss: 1.0771, Val Acc: 0.5615\n",
      "Epoch 37/200\n",
      "Train Loss: 1.0164, Val Loss: 1.0639, Val Acc: 0.5586\n",
      "Epoch 38/200\n",
      "Train Loss: 1.0133, Val Loss: 1.0662, Val Acc: 0.5548\n",
      "Epoch 39/200\n",
      "Train Loss: 1.0103, Val Loss: 1.0727, Val Acc: 0.5575\n",
      "Epoch 40/200\n",
      "Train Loss: 1.0094, Val Loss: 1.0680, Val Acc: 0.5573\n",
      "Epoch 41/200\n",
      "Train Loss: 1.0074, Val Loss: 1.0731, Val Acc: 0.5544\n",
      "Epoch 42/200\n",
      "Train Loss: 1.0072, Val Loss: 1.0623, Val Acc: 0.5598\n",
      "Epoch 43/200\n",
      "Train Loss: 1.0015, Val Loss: 1.0616, Val Acc: 0.5572\n",
      "Epoch 44/200\n",
      "Train Loss: 1.0023, Val Loss: 1.0666, Val Acc: 0.5594\n",
      "Epoch 45/200\n",
      "Train Loss: 1.0009, Val Loss: 1.0933, Val Acc: 0.5454\n",
      "Epoch 46/200\n",
      "Train Loss: 1.0024, Val Loss: 1.0731, Val Acc: 0.5544\n",
      "Epoch 47/200\n",
      "Train Loss: 1.0006, Val Loss: 1.0678, Val Acc: 0.5563\n",
      "Epoch 48/200\n",
      "Train Loss: 0.9948, Val Loss: 1.0663, Val Acc: 0.5574\n",
      "Epoch 49/200\n",
      "Train Loss: 0.9935, Val Loss: 1.0694, Val Acc: 0.5531\n",
      "Epoch 50/200\n",
      "Train Loss: 0.9916, Val Loss: 1.0700, Val Acc: 0.5646\n",
      "Epoch 51/200\n",
      "Train Loss: 0.9892, Val Loss: 1.0727, Val Acc: 0.5530\n",
      "Epoch 52/200\n",
      "Train Loss: 0.9894, Val Loss: 1.0728, Val Acc: 0.5586\n",
      "Epoch 53/200\n",
      "Train Loss: 0.9868, Val Loss: 1.0791, Val Acc: 0.5559\n",
      "Epoch 54/200\n",
      "Train Loss: 0.9860, Val Loss: 1.0694, Val Acc: 0.5605\n",
      "Epoch 55/200\n",
      "Train Loss: 0.9861, Val Loss: 1.0749, Val Acc: 0.5529\n",
      "Epoch 56/200\n",
      "Train Loss: 0.9837, Val Loss: 1.0738, Val Acc: 0.5602\n",
      "Epoch 57/200\n",
      "Train Loss: 0.9835, Val Loss: 1.0818, Val Acc: 0.5479\n",
      "Epoch 58/200\n",
      "Train Loss: 0.9808, Val Loss: 1.0794, Val Acc: 0.5537\n",
      "Epoch 59/200\n",
      "Train Loss: 0.9797, Val Loss: 1.0819, Val Acc: 0.5554\n",
      "Epoch 60/200\n",
      "Train Loss: 0.9772, Val Loss: 1.0801, Val Acc: 0.5559\n",
      "Epoch 61/200\n",
      "Train Loss: 0.9758, Val Loss: 1.0817, Val Acc: 0.5525\n",
      "Epoch 62/200\n",
      "Train Loss: 0.9763, Val Loss: 1.0881, Val Acc: 0.5507\n",
      "Epoch 63/200\n",
      "Train Loss: 0.9757, Val Loss: 1.0819, Val Acc: 0.5521\n",
      "Epoch 64/200\n",
      "Train Loss: 0.9730, Val Loss: 1.0812, Val Acc: 0.5528\n",
      "Epoch 65/200\n",
      "Train Loss: 0.9679, Val Loss: 1.0845, Val Acc: 0.5514\n",
      "Epoch 66/200\n",
      "Train Loss: 0.9691, Val Loss: 1.0832, Val Acc: 0.5544\n",
      "Epoch 67/200\n",
      "Train Loss: 0.9667, Val Loss: 1.0838, Val Acc: 0.5507\n",
      "Epoch 68/200\n",
      "Train Loss: 0.9637, Val Loss: 1.0816, Val Acc: 0.5486\n",
      "Epoch 69/200\n",
      "Train Loss: 0.9637, Val Loss: 1.0938, Val Acc: 0.5480\n",
      "Epoch 70/200\n",
      "Train Loss: 0.9612, Val Loss: 1.0977, Val Acc: 0.5485\n",
      "Epoch 71/200\n",
      "Train Loss: 0.9615, Val Loss: 1.0821, Val Acc: 0.5542\n",
      "Epoch 72/200\n",
      "Train Loss: 0.9578, Val Loss: 1.0960, Val Acc: 0.5517\n",
      "Epoch 73/200\n",
      "Train Loss: 0.9575, Val Loss: 1.1012, Val Acc: 0.5488\n",
      "Epoch 74/200\n",
      "Train Loss: 0.9562, Val Loss: 1.0985, Val Acc: 0.5518\n",
      "Epoch 75/200\n",
      "Train Loss: 0.9543, Val Loss: 1.0973, Val Acc: 0.5416\n",
      "Epoch 76/200\n",
      "Train Loss: 0.9517, Val Loss: 1.0892, Val Acc: 0.5541\n",
      "Epoch 77/200\n",
      "Train Loss: 0.9509, Val Loss: 1.1045, Val Acc: 0.5434\n",
      "Epoch 78/200\n",
      "Train Loss: 0.9501, Val Loss: 1.0962, Val Acc: 0.5531\n",
      "Epoch 79/200\n",
      "Train Loss: 0.9454, Val Loss: 1.1041, Val Acc: 0.5462\n",
      "Epoch 80/200\n",
      "Train Loss: 0.9485, Val Loss: 1.1108, Val Acc: 0.5569\n",
      "Epoch 81/200\n",
      "Train Loss: 0.9450, Val Loss: 1.0952, Val Acc: 0.5473\n",
      "Epoch 82/200\n",
      "Train Loss: 0.9435, Val Loss: 1.1001, Val Acc: 0.5496\n",
      "Epoch 83/200\n",
      "Train Loss: 0.9433, Val Loss: 1.0997, Val Acc: 0.5517\n",
      "Epoch 84/200\n",
      "Train Loss: 0.9398, Val Loss: 1.0997, Val Acc: 0.5482\n",
      "Epoch 85/200\n",
      "Train Loss: 0.9403, Val Loss: 1.1150, Val Acc: 0.5465\n",
      "Epoch 86/200\n",
      "Train Loss: 0.9378, Val Loss: 1.1030, Val Acc: 0.5501\n",
      "Epoch 87/200\n",
      "Train Loss: 0.9330, Val Loss: 1.1070, Val Acc: 0.5502\n",
      "Epoch 88/200\n",
      "Train Loss: 0.9311, Val Loss: 1.1085, Val Acc: 0.5514\n",
      "Epoch 89/200\n",
      "Train Loss: 0.9320, Val Loss: 1.1171, Val Acc: 0.5497\n",
      "Epoch 90/200\n",
      "Train Loss: 0.9297, Val Loss: 1.1172, Val Acc: 0.5428\n",
      "Epoch 91/200\n",
      "Train Loss: 0.9285, Val Loss: 1.1116, Val Acc: 0.5460\n",
      "Epoch 92/200\n",
      "Train Loss: 0.9290, Val Loss: 1.1108, Val Acc: 0.5452\n",
      "Epoch 93/200\n",
      "Train Loss: 0.9236, Val Loss: 1.1249, Val Acc: 0.5379\n",
      "Epoch 94/200\n",
      "Train Loss: 0.9221, Val Loss: 1.1160, Val Acc: 0.5462\n",
      "Epoch 95/200\n",
      "Train Loss: 0.9202, Val Loss: 1.1212, Val Acc: 0.5448\n",
      "Epoch 96/200\n",
      "Train Loss: 0.9188, Val Loss: 1.1243, Val Acc: 0.5438\n",
      "Epoch 97/200\n",
      "Train Loss: 0.9155, Val Loss: 1.1151, Val Acc: 0.5492\n",
      "Epoch 98/200\n",
      "Train Loss: 0.9159, Val Loss: 1.1272, Val Acc: 0.5415\n",
      "Epoch 99/200\n",
      "Train Loss: 0.9147, Val Loss: 1.1232, Val Acc: 0.5471\n",
      "Epoch 100/200\n",
      "Train Loss: 0.9116, Val Loss: 1.1212, Val Acc: 0.5441\n",
      "Epoch 101/200\n",
      "Train Loss: 0.9125, Val Loss: 1.1311, Val Acc: 0.5395\n",
      "Epoch 102/200\n",
      "Train Loss: 0.9066, Val Loss: 1.1282, Val Acc: 0.5455\n",
      "Epoch 103/200\n",
      "Train Loss: 0.9063, Val Loss: 1.1309, Val Acc: 0.5438\n",
      "Epoch 104/200\n",
      "Train Loss: 0.9037, Val Loss: 1.1334, Val Acc: 0.5459\n",
      "Epoch 105/200\n",
      "Train Loss: 0.9052, Val Loss: 1.1346, Val Acc: 0.5462\n",
      "Epoch 106/200\n",
      "Train Loss: 0.9031, Val Loss: 1.1404, Val Acc: 0.5356\n",
      "Epoch 107/200\n",
      "Train Loss: 0.8999, Val Loss: 1.1418, Val Acc: 0.5461\n",
      "Epoch 108/200\n",
      "Train Loss: 0.9014, Val Loss: 1.1370, Val Acc: 0.5433\n",
      "Epoch 109/200\n",
      "Train Loss: 0.8967, Val Loss: 1.1432, Val Acc: 0.5374\n",
      "Epoch 110/200\n",
      "Train Loss: 0.8970, Val Loss: 1.1537, Val Acc: 0.5273\n",
      "Epoch 111/200\n",
      "Train Loss: 0.8954, Val Loss: 1.1486, Val Acc: 0.5385\n",
      "Epoch 112/200\n",
      "Train Loss: 0.8895, Val Loss: 1.1539, Val Acc: 0.5388\n",
      "Epoch 113/200\n",
      "Train Loss: 0.8941, Val Loss: 1.1456, Val Acc: 0.5423\n",
      "Epoch 114/200\n",
      "Train Loss: 0.8870, Val Loss: 1.1555, Val Acc: 0.5431\n",
      "Epoch 115/200\n",
      "Train Loss: 0.8848, Val Loss: 1.1637, Val Acc: 0.5269\n",
      "Epoch 116/200\n",
      "Train Loss: 0.8859, Val Loss: 1.1444, Val Acc: 0.5414\n",
      "Epoch 117/200\n",
      "Train Loss: 0.8837, Val Loss: 1.1540, Val Acc: 0.5394\n",
      "Epoch 118/200\n",
      "Train Loss: 0.8808, Val Loss: 1.1620, Val Acc: 0.5347\n",
      "Epoch 119/200\n",
      "Train Loss: 0.8797, Val Loss: 1.1595, Val Acc: 0.5347\n",
      "Epoch 120/200\n",
      "Train Loss: 0.8759, Val Loss: 1.1653, Val Acc: 0.5362\n",
      "Epoch 121/200\n",
      "Train Loss: 0.8791, Val Loss: 1.1574, Val Acc: 0.5392\n",
      "Epoch 122/200\n",
      "Train Loss: 0.8744, Val Loss: 1.1664, Val Acc: 0.5403\n",
      "Epoch 123/200\n",
      "Train Loss: 0.8758, Val Loss: 1.1720, Val Acc: 0.5386\n",
      "Epoch 124/200\n",
      "Train Loss: 0.8751, Val Loss: 1.1689, Val Acc: 0.5369\n",
      "Epoch 125/200\n",
      "Train Loss: 0.8703, Val Loss: 1.1707, Val Acc: 0.5355\n",
      "Epoch 126/200\n",
      "Train Loss: 0.8693, Val Loss: 1.1976, Val Acc: 0.5029\n",
      "Epoch 127/200\n",
      "Train Loss: 0.8649, Val Loss: 1.1758, Val Acc: 0.5341\n",
      "Epoch 128/200\n",
      "Train Loss: 0.8668, Val Loss: 1.1823, Val Acc: 0.5335\n",
      "Epoch 129/200\n",
      "Train Loss: 0.8606, Val Loss: 1.1765, Val Acc: 0.5352\n",
      "Epoch 130/200\n",
      "Train Loss: 0.8623, Val Loss: 1.1885, Val Acc: 0.5333\n",
      "Epoch 131/200\n",
      "Train Loss: 0.8618, Val Loss: 1.1770, Val Acc: 0.5386\n",
      "Epoch 132/200\n",
      "Train Loss: 0.8626, Val Loss: 1.1822, Val Acc: 0.5397\n",
      "Epoch 133/200\n",
      "Train Loss: 0.8576, Val Loss: 1.1949, Val Acc: 0.5355\n",
      "Epoch 134/200\n",
      "Train Loss: 0.8528, Val Loss: 1.1867, Val Acc: 0.5323\n",
      "Epoch 135/200\n",
      "Train Loss: 0.8540, Val Loss: 1.1891, Val Acc: 0.5340\n",
      "Epoch 136/200\n",
      "Train Loss: 0.8511, Val Loss: 1.1935, Val Acc: 0.5327\n",
      "Epoch 137/200\n",
      "Train Loss: 0.8485, Val Loss: 1.1909, Val Acc: 0.5342\n",
      "Epoch 138/200\n",
      "Train Loss: 0.8524, Val Loss: 1.2006, Val Acc: 0.5309\n",
      "Epoch 139/200\n",
      "Train Loss: 0.8453, Val Loss: 1.2061, Val Acc: 0.5350\n",
      "Epoch 140/200\n",
      "Train Loss: 0.8446, Val Loss: 1.2012, Val Acc: 0.5303\n",
      "Epoch 141/200\n",
      "Train Loss: 0.8451, Val Loss: 1.2138, Val Acc: 0.5371\n",
      "Epoch 142/200\n",
      "Train Loss: 0.8464, Val Loss: 1.2064, Val Acc: 0.5318\n",
      "Epoch 143/200\n",
      "Train Loss: 0.8424, Val Loss: 1.2134, Val Acc: 0.5273\n",
      "Epoch 144/200\n",
      "Train Loss: 0.8415, Val Loss: 1.2055, Val Acc: 0.5268\n",
      "Epoch 145/200\n",
      "Train Loss: 0.8369, Val Loss: 1.2061, Val Acc: 0.5293\n",
      "Epoch 146/200\n",
      "Train Loss: 0.8331, Val Loss: 1.2117, Val Acc: 0.5326\n",
      "Epoch 147/200\n",
      "Train Loss: 0.8363, Val Loss: 1.2168, Val Acc: 0.5332\n",
      "Epoch 148/200\n",
      "Train Loss: 0.8328, Val Loss: 1.2252, Val Acc: 0.5275\n",
      "Epoch 149/200\n",
      "Train Loss: 0.8345, Val Loss: 1.2232, Val Acc: 0.5328\n",
      "Epoch 150/200\n",
      "Train Loss: 0.8310, Val Loss: 1.2258, Val Acc: 0.5266\n",
      "Epoch 151/200\n",
      "Train Loss: 0.8251, Val Loss: 1.2360, Val Acc: 0.5292\n",
      "Epoch 152/200\n",
      "Train Loss: 0.8239, Val Loss: 1.2329, Val Acc: 0.5322\n",
      "Epoch 153/200\n",
      "Train Loss: 0.8247, Val Loss: 1.2241, Val Acc: 0.5363\n",
      "Epoch 154/200\n",
      "Train Loss: 0.8217, Val Loss: 1.2295, Val Acc: 0.5292\n",
      "Epoch 155/200\n",
      "Train Loss: 0.8174, Val Loss: 1.2320, Val Acc: 0.5264\n",
      "Epoch 156/200\n",
      "Train Loss: 0.8208, Val Loss: 1.2371, Val Acc: 0.5264\n",
      "Epoch 157/200\n",
      "Train Loss: 0.8139, Val Loss: 1.2365, Val Acc: 0.5285\n",
      "Epoch 158/200\n",
      "Train Loss: 0.8208, Val Loss: 1.2338, Val Acc: 0.5247\n",
      "Epoch 159/200\n",
      "Train Loss: 0.8211, Val Loss: 1.2423, Val Acc: 0.5242\n",
      "Epoch 160/200\n",
      "Train Loss: 0.8122, Val Loss: 1.2453, Val Acc: 0.5235\n",
      "Epoch 161/200\n",
      "Train Loss: 0.8134, Val Loss: 1.2395, Val Acc: 0.5275\n",
      "Epoch 162/200\n",
      "Train Loss: 0.8132, Val Loss: 1.2458, Val Acc: 0.5292\n",
      "Epoch 163/200\n",
      "Train Loss: 0.8116, Val Loss: 1.2527, Val Acc: 0.5169\n",
      "Epoch 164/200\n",
      "Train Loss: 0.8098, Val Loss: 1.2478, Val Acc: 0.5298\n",
      "Epoch 165/200\n",
      "Train Loss: 0.8073, Val Loss: 1.2560, Val Acc: 0.5201\n",
      "Epoch 166/200\n",
      "Train Loss: 0.8074, Val Loss: 1.2563, Val Acc: 0.5247\n",
      "Epoch 167/200\n",
      "Train Loss: 0.8063, Val Loss: 1.2637, Val Acc: 0.5286\n",
      "Epoch 168/200\n",
      "Train Loss: 0.7977, Val Loss: 1.2667, Val Acc: 0.5213\n",
      "Epoch 169/200\n",
      "Train Loss: 0.7988, Val Loss: 1.2625, Val Acc: 0.5261\n",
      "Epoch 170/200\n",
      "Train Loss: 0.7938, Val Loss: 1.2685, Val Acc: 0.5206\n",
      "Epoch 171/200\n",
      "Train Loss: 0.7953, Val Loss: 1.2746, Val Acc: 0.5254\n",
      "Epoch 172/200\n",
      "Train Loss: 0.7984, Val Loss: 1.2636, Val Acc: 0.5259\n",
      "Epoch 173/200\n",
      "Train Loss: 0.7960, Val Loss: 1.2777, Val Acc: 0.5255\n",
      "Epoch 174/200\n",
      "Train Loss: 0.7917, Val Loss: 1.2847, Val Acc: 0.5235\n",
      "Epoch 175/200\n",
      "Train Loss: 0.7869, Val Loss: 1.2731, Val Acc: 0.5235\n",
      "Epoch 176/200\n",
      "Train Loss: 0.7878, Val Loss: 1.2810, Val Acc: 0.5221\n",
      "Epoch 177/200\n",
      "Train Loss: 0.7883, Val Loss: 1.2955, Val Acc: 0.5252\n",
      "Epoch 178/200\n",
      "Train Loss: 0.7844, Val Loss: 1.2876, Val Acc: 0.5220\n",
      "Epoch 179/200\n",
      "Train Loss: 0.7844, Val Loss: 1.2868, Val Acc: 0.5200\n",
      "Epoch 180/200\n",
      "Train Loss: 0.7875, Val Loss: 1.2951, Val Acc: 0.5237\n",
      "Epoch 181/200\n",
      "Train Loss: 0.7849, Val Loss: 1.2941, Val Acc: 0.5191\n",
      "Epoch 182/200\n",
      "Train Loss: 0.7801, Val Loss: 1.2993, Val Acc: 0.5122\n",
      "Epoch 183/200\n",
      "Train Loss: 0.7804, Val Loss: 1.3093, Val Acc: 0.5104\n",
      "Epoch 184/200\n",
      "Train Loss: 0.7770, Val Loss: 1.3057, Val Acc: 0.5107\n",
      "Epoch 185/200\n",
      "Train Loss: 0.7764, Val Loss: 1.2998, Val Acc: 0.5198\n",
      "Epoch 186/200\n",
      "Train Loss: 0.7710, Val Loss: 1.3016, Val Acc: 0.5167\n",
      "Epoch 187/200\n",
      "Train Loss: 0.7746, Val Loss: 1.3006, Val Acc: 0.5218\n",
      "Epoch 188/200\n",
      "Train Loss: 0.7708, Val Loss: 1.3119, Val Acc: 0.5152\n",
      "Epoch 189/200\n",
      "Train Loss: 0.7711, Val Loss: 1.3109, Val Acc: 0.5157\n",
      "Epoch 190/200\n",
      "Train Loss: 0.7682, Val Loss: 1.3156, Val Acc: 0.5156\n",
      "Epoch 191/200\n",
      "Train Loss: 0.7629, Val Loss: 1.3170, Val Acc: 0.5202\n",
      "Epoch 192/200\n",
      "Train Loss: 0.7677, Val Loss: 1.3219, Val Acc: 0.5206\n",
      "Epoch 193/200\n",
      "Train Loss: 0.7677, Val Loss: 1.3260, Val Acc: 0.5228\n",
      "Epoch 194/200\n",
      "Train Loss: 0.7613, Val Loss: 1.3263, Val Acc: 0.5218\n",
      "Epoch 195/200\n",
      "Train Loss: 0.7652, Val Loss: 1.3345, Val Acc: 0.5180\n",
      "Epoch 196/200\n",
      "Train Loss: 0.7619, Val Loss: 1.3275, Val Acc: 0.5202\n",
      "Epoch 197/200\n",
      "Train Loss: 0.7637, Val Loss: 1.3305, Val Acc: 0.5197\n",
      "Epoch 198/200\n",
      "Train Loss: 0.7545, Val Loss: 1.3357, Val Acc: 0.5136\n",
      "Epoch 199/200\n",
      "Train Loss: 0.7570, Val Loss: 1.3464, Val Acc: 0.5252\n",
      "Epoch 200/200\n",
      "Train Loss: 0.7638, Val Loss: 1.3371, Val Acc: 0.5194\n",
      "\n",
      "Sample predictions:\n",
      "Predicted: ['Dragons' 'Herald' 'Herald' 'Minions' 'Gold' 'Gold' 'Herald' 'Gold'\n",
      " 'Herald' 'Minions' 'Herald' 'Herald' 'Minions' 'Herald' 'Gold' 'Herald'\n",
      " 'Gold' 'Herald' 'Gold' 'Towers' 'Minions' 'Kills' 'Herald' 'Herald'\n",
      " 'Gold' 'Dragons' 'Dragons' 'Gold' 'Gold' 'Gold' 'Minions' 'Herald'\n",
      " 'Herald' 'Minions' 'Herald' 'Minions' 'Herald' 'Gold' 'Gold' 'Gold'\n",
      " 'Herald' 'Minions' 'Kills' 'Gold' 'Minions' 'Dragons' 'Gold' 'Minions'\n",
      " 'Minions' 'Kills' 'Gold' 'Kills' 'Herald' 'Herald' 'Herald' 'Herald'\n",
      " 'Gold' 'Minions' 'Minions' 'Gold' 'Minions' 'Herald' 'Herald' 'Minions']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class GoldLabelRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=1):\n",
    "        super(GoldLabelRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # RNN layer (could also use LSTM or GRU)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def train_model(train_loader, test_loader, input_size, num_classes, device='cpu'):\n",
    "    # Hyperparameters\n",
    "    hidden_size = 128\n",
    "    num_layers = 4\n",
    "    learning_rate = 0.05\n",
    "    num_epochs = 50\n",
    "    \n",
    "    # Initialize model\n",
    "    model = GoldLabelRNN(input_size, hidden_size, num_classes, num_layers).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_true.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(test_loader)\n",
    "        val_acc = accuracy_score(all_true, all_preds)\n",
    "        \n",
    "        # print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        # print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Get processed data\n",
    "    tensor_data = prepare_rnn_data(train_test_dict, batch_size=64)\n",
    "    \n",
    "    # Select a subset (e.g., 20% completion data)\n",
    "    subset = '60'\n",
    "    train_loader = tensor_data[subset]['train_loader']\n",
    "    test_loader = tensor_data[subset]['test_loader']\n",
    "    label_encoder = tensor_data[subset]['label_encoder']\n",
    "    \n",
    "    # Determine model parameters\n",
    "    input_size = train_test_dict[subset]['X_train'].shape[1]  # Number of features\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    # Train model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = train_model(train_loader, test_loader, input_size, num_classes, device)\n",
    "    \n",
    "    # Example prediction\n",
    "    sample_batch, _ = next(iter(test_loader))\n",
    "    sample_batch = sample_batch.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(sample_batch)\n",
    "        _, predicted_labels = torch.max(predictions, 1)\n",
    "        \n",
    "    print(\"\\nSample predictions:\")\n",
    "    print(\"Predicted:\", label_encoder.inverse_transform(predicted_labels.cpu().numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
